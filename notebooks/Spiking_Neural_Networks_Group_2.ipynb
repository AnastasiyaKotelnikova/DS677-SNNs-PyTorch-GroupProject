# DS677-Group Project

# Phase 1: Dataset Preparation & Model Implementation:

# 1. Finalize dataset selection and preprocessing pipeline
# 2. Implement PyTorch DataLoader for efficient data handling
# 3. Explore dataset visualization and feature extraction
# 4. Investigate suitable SNN architectures
# 5. Implement and train the first SNN model using Norse
# 6. Analyze initial results and log performance metrics

!pip install norse

# Import Required Libraries

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import h5py
import numpy as np

# Norse for spiking models
import norse.torch as norse
# Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
**1. Finalize dataset selection and preprocessing pipeline**
**Download SHD Dataset:**

This step downloads the Spiking Heidelberg Digits (SHD) dataset directly into the working directory using wget. It includes:

shd_train.h5.gz: The compressed training dataset

shd_test.h5.gz: The compressed testing dataset

These files are required for loading and preprocessing spike-based neural data in later steps.
# Download SHD training and test sets directly into /content/
!wget -O /content/shd_train.h5.gz https://compneuro.net/datasets/shd_train.h5.gz
!wget -O /content/shd_test.h5.gz https://compneuro.net/datasets/shd_test.h5.gz

**Load and Inspect SHD Dataset Structure**
This block performs three key tasks:

**1. Confirms File Access:**
Opens the shd_train.h5 file to verify that it exists and displays its top-level keys.
Expected keys: 'extra', 'labels', and 'spikes'.

**2. Inspects Data Structure:**
Displays the keys under 'spikes' to check how spike data is stored ('times', 'units').
Verifies that 'labels' contains 8156 samples, confirming the dataset size.

**3. Loads Data into Memory:**
Extracts the spike times, spike units, and labels from the HDF5 file for further processing and training.
# Confirms the file is loaded correctly

file_path = '/content/shd_train.h5'
with h5py.File(file_path, 'r') as f:
    print("Available keys:", list(f.keys()))

# Confirms the label shape → 8156, indicating number of samples

with h5py.File(file_path, 'r') as f:
    print("Spikes group keys:", list(f['spikes'].keys()))
    print("Labels dataset shape:", f['labels'].shape)

# Loads the data into memory

with h5py.File(file_path, 'r') as f:
    spikes_times = f['spikes']['times'][:]
    spikes_units = f['spikes']['units'][:]
    labels = f['labels'][:]

**2. Implement PyTorch DataLoader for efficient data handling**
**This custom SHDDataset class allows integration of the Spiking Heidelberg Digits (SHD) dataset into a PyTorch pipeline:**
# Define Custom Dataset Loader

class SHDDataset(Dataset):
    def __init__(self, file_path):
        self.file = h5py.File(file_path, 'r')
        self.spike_times = self.file['times']
        self.spike_units = self.file['spikes']
        self.labels = self.file['labels']

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'spike_times': self.spike_times[idx],
            'spike_units': self.spike_units[idx],
            'label': self.labels[idx]
        }

**3. Explore dataset visualization and feature extraction**
**This shd_collate_fn function is designed to help PyTorch’s DataLoader handle variable-length spiking data sequences.**
# Define a Custom Collate Function for Uneven Sequences

def shd_collate_fn(batch):
    spike_times = [item['spike_times'] for item in batch]
    spike_units = [item['spike_units'] for item in batch]
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)

    return {
        'spike_times': spike_times,
        'spike_units': spike_units,
        'label': labels
    }

**This function convert_to_spike_tensor transforms raw spike event data (times and units) into a 3D PyTorch tensor format suitable for feeding into a spiking neural network (SNN).**
# Convert Spikes to a Tensor

def convert_to_spike_tensor(spike_times, spike_units, batch_size, input_size, duration, dt):
    timesteps = int(duration / dt)
    spike_tensor = torch.zeros((timesteps, batch_size, input_size), dtype=torch.float32)

    for b in range(batch_size):
        times = spike_times[b]
        units = spike_units[b]
        time_bins = (times / dt).astype(int)

        for t, u in zip(time_bins, units):
            if t < timesteps and u < input_size:
                spike_tensor[t, b, u] = 1.0
    return spike_tensor

**4.  Investigate suitable SNN architectures**
**This SimpleSNN class defines a basic two-layer spiking neural network leveraging the Norse library, which integrates biologically inspired spiking neuron models into PyTorch.**

**Norse enables temporal dynamics in neurons, allowing this model to learn from the timing of spikes, not just their count — essential for working with spiking datasets like SHD.**
**bold text**

# Define the SNN Model with Norse

class SimpleSNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(700, 128)
        self.lif1 = norse.LIFCell()
        self.fc2 = nn.Linear(128, 20)  # 20 output classes
        self.lif2 = norse.LIFCell()

    def forward(self, x):
        seq_length, batch_size, _ = x.shape
        lif1_state = self.lif1.state
        lif2_state = self.lif2.state
        outputs = []

        for t in range(seq_length):
            z = self.fc1(x[t])
            z, lif1_state = self.lif1(z, lif1_state)
            z = self.fc2(z)
            z, lif2_state = self.lif2(z, lif2_state)
            outputs.append(z)

        return torch.stack(outputs)

**This cell opens the SHD training file using h5py and explores the 'spikes' group, which contains the spiking event data for all samples.**
with h5py.File('/content/shd_train.h5', 'r') as f:
    spike_group = f['spikes']
    keys = list(spike_group.keys())
    print("First 10 keys in spikes group:", keys[:10])

**This cell defines a custom SHDDataset class, which extends PyTorch's Dataset # interface. It allows efficient indexed access to the SHD dataset stored in HDF5 format, and converts: 'spikes' into a PyTorch tensor of spike data 'label' into a PyTorch tensor of class labels.
This class is required for integration with DataLoader in PyTorch.**
# Custom Dataset Class for SHD (.h5) File

class SHDDataset(Dataset):
    def __init__(self, file_path):
        self.file_path = file_path
        with h5py.File(file_path, 'r') as f:
            self.length = len(f['spikes'].keys())
            self.keys = list(f['spikes'].keys())
            self.labels = f['labels'][:]

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        with h5py.File(self.file_path, 'r') as f:
            key = self.keys[idx]
            spike_data = f['spikes'][key][:]
            label = self.labels[idx]
        return {
            'spikes': torch.tensor(spike_data, dtype=torch.float32),
            'label': torch.tensor(label, dtype=torch.long)
        }

**This code loads the core spiking data from the SHD dataset:**

f['spikes']['times'][:] loads the spike timing information (when each spike occurred).

f['spikes']['units'][:] loads the spike unit/neuron identifiers (which neuron fired).

f['labels'][:] loads the class labels (spoken digit for each sample).


**Outputs:**

Each of these arrays has shape (8156,), meaning the dataset contains 8156 spike events and corresponding labels — confirming consistency across features and targets.
# Extract and Inspect 'times' and 'units' Data

file_path = '/content/shd_train.h5'

with h5py.File(file_path, 'r') as f:
    times = f['spikes']['times'][:]
    units = f['spikes']['units'][:]
    labels = f['labels'][:]

print(f"Times shape: {times.shape}")
print(f"Units shape: {units.shape}")
print(f"Labels shape: {labels.shape}")

**This code defines a reusable custom SHDDataset class that extends PyTorch’s Dataset.**

__init__: Loads the spike timing (times), neuron units (units), and labels from the SHD .h5 file into memory.

__len__: Returns the number of samples in the dataset (based on number of labels).

__getitem__: Allows indexing like dataset[i] to return a dictionary containing:

'spike_times'

'spike_units'

'label'
# Wrap It Into a Custom Dataset Class

from torch.utils.data import Dataset

class SHDDataset(Dataset):
    def __init__(self, file_path):
        with h5py.File(file_path, 'r') as f:
            self.spike_times = f['spikes']['times'][:]
            self.spike_units = f['spikes']['units'][:]
            self.labels = f['labels'][:]

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'spike_times': self.spike_times[idx],
            'spike_units': self.spike_units[idx],
            'label': self.labels[idx]
        }

**This shd_collate_fn function is designed for use with a PyTorch DataLoader to collate batches of SHD data where samples have variable spike lengths (common in event-based datasets).**

For each batch:

spike_times and spike_units are collected as lists (not tensors) because their lengths vary.

labels are converted into a fixed-size tensor (1D integer tensor) using torch.tensor(...).
#  Define a Custom Collate Function
# This handles batches where each sample might have different spike lengths

def shd_collate_fn(batch):
    spike_times = [item['spike_times'] for item in batch]
    spike_units = [item['spike_units'] for item in batch]
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
    return {
        'spike_times': spike_times,
        'spike_units': spike_units,
        'label': labels
    }

**In this step, we create a DataLoader to efficiently iterate over the SHD dataset:**

**train_dataset:** Created using our custom SHDDataset class which loads spike data and labels from the SHD training file.

**train_loader:** Uses PyTorch DataLoader with:

- batch_size=32: Fetches 32 samples per batch

- shuffle=True: Ensures randomization during each epoch

- collate_fn=shd_collate_fn: Custom function to handle variable-length sequences.

**Outputs:**

The spike_times list has 32 elements → one per sample.

The label tensor has shape [32] → corresponding class labels.

This confirms our custom dataset and DataLoader are working correctly.
# Create DataLoader

train_dataset = SHDDataset('/content/shd_train.h5')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=shd_collate_fn)

batch = next(iter(train_loader))
print(type(batch['spike_times']), len(batch['spike_times']))
print(batch['label'].shape)

**This function transforms raw spike timing and unit data into a 3D tensor representation suitable for input into a Spiking Neural Network (SNN).**

**Output: **

A tensor ready for SNN input with shape [timesteps, batch_size, input_size].
# Convert Spikes into a Tensor Format for Model Input

def convert_to_spike_tensor(spike_times, spike_units, batch_size, input_size, duration, dt):
    timesteps = int(duration / dt)
    spike_tensor = torch.zeros((timesteps, batch_size, input_size), dtype=torch.float32)

    for b in range(len(spike_times)):
        times = spike_times[b]
        units = spike_units[b]
        time_bins = (times / dt).astype(int)

        for t, u in zip(time_bins, units):
            if t < timesteps and u < input_size:
                spike_tensor[t, b, u] = 1.0

    return spike_tensor

**This cell defines a custom SNNModel using the Norse library, which integrates biologically-inspired spiking neuron models into PyTorch.**

Output:
Returns a stacked tensor of output spikes over all time steps.
# Define a Spiking Neural Network Model with Norse

class SNNModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.lif1 = norse.LIFCell()

        self.fc2 = nn.Linear(hidden_size, output_size)
        self.lif2 = norse.LIFCell()

    def forward(self, x):
        seq_length, batch_size, _ = x.shape
        spk1, mem1 = None, None
        spk2, mem2 = None, None
        output_spikes = []

        for t in range(seq_length):
            z1 = self.fc1(x[t])
            spk1, mem1 = self.lif1(z1, mem1)

            z2 = self.fc2(spk1)
            spk2, mem2 = self.lif2(z2, mem2)

            output_spikes.append(spk2)

        return torch.stack(output_spikes)

**5. Implement and train the first SNN model using Norse**
**This cell sets up everything needed to train the Spiking Neural Network (SNN):**

Device Configuration

Model Initialization

Loss Function

Optimizer
#  Instantiate the Model & Optimizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Adjust hidden_size and output_size to your case if needed
model = SNNModel(input_size=700, hidden_size=128, output_size=20).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

loss_list = []
**Training the Spiking Neural Network (SNN) Model**:

This section performs model training using the Norse-based spiking neural network (SNN) defined earlier. The training loop runs for 10 epochs and includes the following key steps:

- **Model Preparation:** The model is set to training mode and loss is tracked per epoch.
- **Batch Processing:** Each batch is passed through a custom spike conversion function to generate a 3D tensor representing spike events over time and input units.
- **Forward Pass:** The converted spike tensor is passed through the SNN model, and the output spikes are averaged over time to obtain logits.
- **Loss Computation:** Cross-entropy loss is calculated between predicted logits and ground truth labels.
- **Backpropagation:** The model performs backward propagation and updates weights using the Adam optimizer.
- **Progress Tracking:** The total loss is printed for each epoch, and a decreasing trend indicates successful learning.

**Output:**

The model was trained over 10 epochs.

The loss started at ~763.91 and steadily decreased to ~757.73.

This indicates that the SNN model is learning, and training is working as expected.

The gradual drop in loss reflects improved performance over time.

# Training Loop

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        spike_tensor = convert_to_spike_tensor(
            spike_times=batch['spike_times'],
            spike_units=batch['spike_units'],
            batch_size=len(batch['spike_times']),
            input_size=700,
            duration=100.0,
            dt=1.0
        ).to(device)

        labels = batch['label'].to(device)

        # Forward pass
        outputs = model(spike_tensor)
        logits = outputs.mean(dim=0)  # Average over time

        # Compute loss and backprop
        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}")


torch.save(model.state_dict(), "norse_snn_model.pth")
loss_list = [
    763.9119,
    763.9119,
    763.8953,
    763.6119,
    762.8166,
    761.9438,
    760.8495,
    759.7484,
    758.6771,
    757.7340
]
**6. Analyze initial results and log performance metrics**
**Visualize Training Loss Over Epochs**:

This plot provides a visual representation of how the model's loss changed across training epochs. A downward trend in the loss indicates that the model is learning effectively and improving its predictions over time.

**Output Explanation:**

The line plot shows training loss over 10 epochs.

Each dot (o) represents the total loss at the end of one epoch.

The loss value decreases gradually, from around 763.9 to 757.7 by the 10th epoch.

This decline confirms that your Norse SNN model is learning, even if slowly.

The learning curve appears smooth and consistent, which is a good sign (no overfitting or spikes).

**This loop completes Phase 1 of the project by training the first SNN model on the SHD dataset.**



import matplotlib.pyplot as plt

plt.plot(range(1, 11), loss_list, marker='o', linestyle='-')
plt.title("Training Loss Over Time (Norse SNN Model)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.xticks(range(1, 11))
plt.show()

v
